
1.4. ¿Qué es programación?
Programación es el proceso de tomar un algoritmo y codificarlo en una notación, 
un lenguaje de programación, de modo que pueda ser ejecutado por una computadora. 
Aunque existen muchos lenguajes de programación y muchos tipos diferentes de computadoras, 
el primer paso es la necesidad de tener una solución. Sin un algoritmo no puede haber un programa.

Las ciencias de la programación no son el estudio de la programación. La programación, sin embargo, 
es una parte importante de lo que hace un científico de la computación. La programación es a menudo la manera 
en la que creamos una representación para nuestras soluciones. Por tanto, esta representación en un lenguaje y 
el proceso de crearla se convierte en una parte fundamental de la disciplina.

Los algoritmos describen la solución a un problema en términos de los datos requeridos para representar el caso 
del problema y el conjunto de pasos necesarios para producir el resultado pretendido. Los lenguajes de programación 
deben suministrar un modo notacional para representar tanto el proceso como los datos. Para este fin, los lenguajes 
suministran estructuras de control y tipos de datos.

Las estructuras de control permiten que los pasos algorítmicos sean representados de una manera conveniente 
pero sin ambigüedades. Como mínimo, los algoritmos requieren estructuras que lleven a cabo procesamiento secuencial, 
selección para toma de decisiones e iteraciones para control repetitivo. Siempre y cuando el lenguaje proporcione 
estas instrucciones básicas, éste puede ser usado para la representación del algoritmo.

Todos los ítems de datos en la computadora están representados como cadenas de dígitos binarios. Con el fin de darle 
significado a estas cadenas, necesitamos tener tipos de datos. Los tipos de datos brindan una interpretación para 
estos datos binarios de modo que podamos considerarlos en términos que tengan sentido con respecto al problema que 
está siendo resuelto. Estos tipos de datos incorporados de bajo nivel (a menudo denominados tipos de datos primitivos) proporcionan los bloques constructivos para el desarrollo de algoritmos.

Por ejemplo, la mayoría de lenguajes de programación proporcionan un tipo de datos para los enteros. Las cadenas 
de dígitos binarios en la memoria de la computadora pueden interpretarse como enteros y se les dan los significados
 típicos que comúnmente asociamos con los enteros (e.g. 23, 654 y -19). Además, un tipo de datos también proporciona 
 una descripción de las operaciones en las que los ítems de datos pueden participar. Con enteros, son comunes 
 las operaciones tales como la suma, la resta y la multiplicación. Podemos dar por sentado que los tipos de datos 
 numéricos puedan participar en estas operaciones aritméticas.